{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b0122c-ac40-4a41-a07a-a3fe3b163534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "import numpy as np\n",
    "from transformers import TFAutoModel\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "np.random.seed(21)\n",
    "warnings.filterwarnings('ignore')\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b0bb4c-fb64-4e54-bfff-977d56a32716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "model_name = 'vinai/bertweet-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "MAX_LEN = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2c113d-2ca0-4ae5-989a-83750588164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =pd.read_csv(\"../data/task1a/train.csv\")\n",
    "train[\"NOT\"]= train[\"Hate\"].apply(lambda x: 1 if x == 0 else 0 )\n",
    "train[\"HOF\"]= train[\"Hate\"].apply(lambda x: 1 if x == 1 else 0 )\n",
    "test=pd.read_csv(\"../data/test.csv\")\n",
    "FEATURES=[\"dale_chall\",\"bad_words\", \"num_words\",\"all_caps\",\t\"emoji\"\t,\"capitals\",\"total_length\",\"caps_vs_length\",\"num_unique_words\",\t\"words_vs_unique\"\t,\"num_urls\",\t\"!\",\t\"?\"]\n",
    "features = train[FEATURES].fillna(0)\n",
    "test_features = test[FEATURES].fillna(0)\n",
    "ss = StandardScaler()\n",
    "ss.fit(features)\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb9f607-3e4a-45c0-95c4-a7b15d169bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cutting off the middle part of long texts.\n",
    "def text_process(text):\n",
    "    ws = text.split(' ')\n",
    "    if(len(ws)>130):\n",
    "        text = ' '.join(ws[:90]) + ' ' + ' '.join(ws[-40:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b44765c-8d6a-43cf-a1d9-03dd5058a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[[\"NOT\",\"HOF\"]].values\n",
    "X_train = train['c_text'].apply(lambda x: text_process(str(x))).fillna(\"something\").values.tolist()\n",
    "X_test = test['c_text'].apply(lambda x: text_process(str(x))).fillna(\"something\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820ba564-b870-4d5b-b667-87db6977a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, tokenizer, max_seq_len = 125 ):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "                            sentence,\n",
    "                            truncation=True,               \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = max_seq_len,\n",
    "                    )\n",
    "        \n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return np.array(tokenized_sentences)\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "def regular_encode(texts,tokenizer,maxlen=MAX_LEN):\n",
    "  input_ids = tokenize_sentences(texts, tokenizer, MAX_LEN)\n",
    "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "  attention_masks = create_attention_masks(input_ids)\n",
    "  return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64166b3-b3df-47a0-83d3-37addf045d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec30e461d574f1b94e00850c1ba1b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f023c2d7a149dc9c5124426df2c90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test,x_test_att = regular_encode(X_test, tokenizer, maxlen=MAX_LEN)\n",
    "x_train,x_train_att = regular_encode(X_train,tokenizer,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead653bb-d18e-448d-82e6-31c7e5b7aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ft_1a(bert_model, features ,clipvalue=1.,num_filters=40,dropout=0.5,max_len=125):\n",
    "    import tensorflow as tf\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    attention_masks = Input(shape=(max_len,), dtype=tf.int32, name=\"input_att_masks\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_masks)\n",
    "    hidden_states= bert_output.hidden_states\n",
    "    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "    x= tf.concat(last_four_layers,-1)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(x)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs,axis=1)\n",
    "    x = Dropout(0.5)(l_merge)  \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x,features_input])\n",
    "    outp = Dense(2, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[input_ids,attention_masks,features_input], outputs=outp)\n",
    "    import tensorflow as tf\n",
    "    adam = tf.optimizers.Adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2202a06-8ce1-4912-9135-cf0e8f1bef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 125)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_att_masks (InputLayer)    [(None, 125)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 134899968   input_word_ids[0][0]             \n",
      "                                                                 input_att_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 125, 3072)]  0           tf_roberta_model[0][13]          \n",
      "                                                                 tf_roberta_model[0][11]          \n",
      "                                                                 tf_roberta_model[0][10]          \n",
      "                                                                 tf_roberta_model[0][9]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 124, 128)     786560      tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 123, 128)     1179776     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 122, 128)     1572992     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 121, 128)     1966208     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 41, 128)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 41, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 40, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 40, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 162, 128)     0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 162, 128)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 20736)        0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          2654336     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 141)          0           dropout_38[0][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            284         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 143,060,124\n",
      "Trainable params: 143,060,124\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "fold: 0\n",
      "321/321 [==============================] - 12s 37ms/step\n",
      "fold: 1\n",
      "321/321 [==============================] - 10s 31ms/step\n",
      "fold: 2\n",
      "321/321 [==============================] - 10s 31ms/step\n",
      "fold: 3\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 4\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 5\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 6\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 7\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 8\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "fold: 9\n",
      "321/321 [==============================] - 10s 32ms/step\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "transformer_layer = TFAutoModel.from_pretrained(model_name,output_hidden_states=True)\n",
    "transformer_layer.compile()\n",
    "model = get_model_ft_1a(transformer_layer, features)\n",
    "model.summary()\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "num_folds = 10\n",
    "predict = np.zeros((test.shape[0],2))\n",
    "x_test=np.asarray(x_test).astype(np.float32)\n",
    "for i in range(num_folds):\n",
    "    print(f\"fold: {i}\")\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    model = get_model_ft_1a(transformer_layer, features)\n",
    "    gc.collect()\n",
    "    model.load_weights(f\"/scratch/arjunth2001/ft_fine_1a_cnn_{i+1}.h5\")\n",
    "    predict += model.predict([x_test,x_test_att,test_features], batch_size=BATCH_SIZE,verbose=1) / num_folds\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dad3188-d886-4a44-bcf6-e75e752a993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30eba6b0-b66c-49bf-a404-5638e8eb58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"NOT\",\"HOF\"]]=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c83d4a-e30d-4f27-8f44-0f2ea8293063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"NOT\",\"HOF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "566993d6-0d62-4b2a-a841-e9041f192e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOT</th>\n",
       "      <th>HOF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552159</td>\n",
       "      <td>0.447841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.447820</td>\n",
       "      <td>0.552180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.432632</td>\n",
       "      <td>0.567368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.575164</td>\n",
       "      <td>0.424836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.439542</td>\n",
       "      <td>0.560458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>0.574545</td>\n",
       "      <td>0.425455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.435639</td>\n",
       "      <td>0.564361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0.427052</td>\n",
       "      <td>0.572948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0.447495</td>\n",
       "      <td>0.552505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0.427633</td>\n",
       "      <td>0.572367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1281 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NOT       HOF\n",
       "0     0.552159  0.447841\n",
       "1     0.447820  0.552180\n",
       "2     0.432632  0.567368\n",
       "3     0.575164  0.424836\n",
       "4     0.439542  0.560458\n",
       "...        ...       ...\n",
       "1276  0.574545  0.425455\n",
       "1277  0.435639  0.564361\n",
       "1278  0.427052  0.572948\n",
       "1279  0.447495  0.552505\n",
       "1280  0.427633  0.572367\n",
       "\n",
       "[1281 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "923306f9-ac26-4a52-bbca-7650c8566dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55215941, 0.4478406 ],\n",
       "       [0.44782041, 0.55217962],\n",
       "       [0.43263201, 0.56736799],\n",
       "       ...,\n",
       "       [0.42705164, 0.57294835],\n",
       "       [0.447495  , 0.55250502],\n",
       "       [0.42763308, 0.57236692]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb51508b-74d3-4612-90b4-7dc28d0838b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./predictions/ft_1a_cnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1e946-4e7e-4960-946f-1f4f6891ca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
